{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function Notebook for Insights on the Spatiotemporal Variability of Downslope Winds in Coastal Santa Barbara: A Case Study From the Sundowner Winds Experiment (SWEX)**\n",
    "## This notebook contains the following:\n",
    "> - #### Functions that are used in other Jupyter Notebooks for all analysis and figures in the article Insights on the Spatiotemporal Variability of Downslope Winds in Coastal Santa Barbara: A Case Study From the Sundowner Winds Experiment (SWEX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function: \"cartopy_basemap_subplots\"**\n",
    "\n",
    "- #### Description: A function to draw one or multiple cartopy plots with lat/lon labels and a coastline, as well as numerous other options. This function can be adapted for many areas, but has the greatest amount of options for the Santa Barbara, CA domain. Here is a [link](https://github.com/dlnash/AR_types/blob/master/modules/plotter.py) to a similar function created by [Deanna Nash](https://dlnash.github.io/). Lower resolution (10m) Ocean mask, coastline, and island features are courtesy of [NaturalEarth.com](https://www.naturalearthdata.com/). Note also that all of the [Natural Earth data](https://www.naturalearthdata.com/features/) uses the WGS84 datum, which is the deafult globe used when intializing a Cartopy projection. Thus, we do not need to define a seperate [globe](https://scitools.org.uk/cartopy/docs/latest/reference/generated/cartopy.crs.Globe.html) instance. Higher resoloution Ocean mask and coastline features are courtesy of [FOSSGIS](https://osmdata.openstreetmap.de/). They are processed Open Street Maps shape files. All files reference to the WGS84 datum, which is the default globe for cartopy.\n",
    "\n",
    "> - #### **Input Variables**: \n",
    ">> - #### **plot_crs**: a cartopy coordinate reference system (CRS) that describes the CRS we want the map to be plotted in\n",
    ">> - #### **data_crs**: a cartopy coordinate reference system (CRS) that describes that CRS that our data comes in\n",
    ">> - #### **fig_size**: a tuple that defines the figure size. First value is length and second value is height of figure (in inches).\n",
    ">> - #### **nrows**: An integer which defines how many subplot rows the user wants.\n",
    ">> - #### **ncols**: An integer which defines how many subplot cols the user wants.\n",
    ">> - #### **wspace_float**: A number which defines the width spacing between sublpots.\n",
    ">> - #### **hspace_float**: A number which defines the height spacing between sublpots.\n",
    ">> - #### **lon_lat_extent**: an array-like variable that will set the extent of our map and has the order: [lower_lon, upper_lon, lower_lat, upper_lat]\n",
    ">> - #### **lon_lat_ticks**:  an array-like variable that will be used to draw the ticklabels on our map. Has the order [lower_lon_tick, upper_lon_tick, lower_lat_tick, upper_lat_tick] \n",
    ">> - #### **lon_lat_tick_num**: a list of two integers that tells us how many evenly spaced lon/lat ticklabels we want. Has the order [lon_tick_num, lat_tick_num]\n",
    ">> - #### **high_res_coastline**: A boolean which indicates if we should use the high resolution OSM coastline outline, or the 10m natural earth coastline outline.\n",
    ">> - #### **high_res_wrf_topo_sb_bool**: A boolean which indicates if we should draw 50m resolution WRF topography over Santa Barbara. Only draws topography if \"True\" is provided.\n",
    ">> - #### **low_res_wrf_topo_sb_bool**: A boolean which indicates if we should draw 1km resolution WRF topography over Santa Barbara. Only draws topography if \"True\" is provided.\n",
    ">> - #### **low_res_wrf_topo_ca_bool**: A boolean which indicates if we should draw a ?km resolution WRF topography layer that covers the state of California. Only draws topography if \"True\" is provided.\n",
    ">> - #### **wrf_topo_colorbar_each_plot_bool**: A boolean which indicates if we should draw a colorbar for our WRF topography for each plot or subplot created.\n",
    ">> - #### **wrf_topo_colorbar_entire_figure_bool**: A boolean which indicates if we should draw a single colorbar for our WRF topography for the entire figure (subplots or single plot).\n",
    ">> - #### **scale_bar_bool**: A boolean which indicates if we should draw a scale bar. Only draws scale bar if \"True\" is provided.\n",
    ">> - #### **scale_bar_position**: A tuple that defines the (x,y) coordinate to begin drawing the scale bar.\n",
    ">> - #### **scale_bar_length**: A value that takes an unusual format to define how long to make the scalebar. Examples: 1_0 is 10km, 10_0 is 100km, and 0o1 is 1km.\n",
    ">> - #### **inset_ca_bool**: A boolean which indicates if we should draw an inset figure of California. Only draws inset if \"True\" is provided.\n",
    ">> - #### **inset_bbox_position**: A tuple that defines the bounding box for the inset axis. This usually needs to be reset if you change the extent of your map. Format of tuple is (start_x_position, start_y_position, end_x_position, end_y_position).\n",
    ">> - #### **ocean_color**: A string that defines the color to use for the Natural Earth ocean mask.\n",
    "> - #### **Output Variables**:\n",
    ">> - #### **fig**: matplotlib figure \n",
    ">> - #### **ax or axs**: cartopy axis or axes with basemap data plotted on one or multiple subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /home/sbarc/students/mariandob/mambaforge/envs/swex/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------------\n",
    "#Entire package imports\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "#cartopy imports\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.shapereader as shapereader\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "#matplotlib impots\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "#PIL imports\n",
    "import PIL.Image\n",
    "\n",
    "#scalebar imports\n",
    "import scalebar\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "def cartopy_basemap_subplots(plot_crs, data_crs, fig_size, \n",
    "                             nrows, ncols, wspace_float, hspace_float,\n",
    "                             lon_lat_extent, lon_lat_ticks, lon_lat_tick_num, \n",
    "                             lon_lat_ticks_on=True, xtick_ytick_set_list=[True],\n",
    "                             high_res_coastline=False, \n",
    "                             high_res_wrf_topo_sb_bool=False, \n",
    "                             low_res_wrf_topo_sb_bool=False, \n",
    "                             low_res_wrf_topo_ca_bool=False, \n",
    "                             wrf_topo_colorbar_each_plot_bool=True, \n",
    "                             wrf_topo_colorbar_entire_figure_bool=False,\n",
    "                             scale_bar_bool=False, scale_bar_position=None, scale_bar_length=1_0, \n",
    "                             inset_ca_bool=False, inset_bbox_position=None, \n",
    "                             ocean_color=None):\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "    #Define font items\n",
    "    fontdict_text_color_bar  = {'fontsize': 24, 'fontweight': 'normal', 'fontname': 'Nimbus Roman'}\n",
    "    fontdict_text_annotation = {'fontsize': 24, 'fontweight': 'normal', 'fontname': 'Nimbus Roman'}\n",
    "    fontdict_tick_labels     = {'fontsize': 24, 'fontweight': 'normal', 'fontname': 'Nimbus Roman'}\n",
    "    \n",
    "    #Define both map and data coordinate systems  \n",
    "    plot_crs = plot_crs\n",
    "    data_crs = data_crs\n",
    "    \n",
    "    #Define map boundaries\n",
    "    lon_min_extent = lon_lat_extent[0]\n",
    "    lon_max_extent = lon_lat_extent[1]\n",
    "    lat_min_extent = lon_lat_extent[2]\n",
    "    lat_max_extent = lon_lat_extent[3]\n",
    "    \n",
    "    #Only define lat/lon tick variables if the user requested it\n",
    "    if lon_lat_ticks_on == True:\n",
    "    \n",
    "        #Define map tick labels\n",
    "        lon_min_tick = lon_lat_ticks[0]\n",
    "        lon_max_tick = lon_lat_ticks[1]\n",
    "        lat_min_tick = lon_lat_ticks[2]\n",
    "        lat_max_tick = lon_lat_ticks[3]\n",
    "\n",
    "        #Number of ticks we want. Serves as input to numpy \"linspace\" function below\n",
    "        lon_num_tick  = lon_lat_tick_num[0]\n",
    "        lat_num_tick  = lon_lat_tick_num[1]\n",
    "\n",
    "        #Define longitude and latitude ticks using \"linspace\"\n",
    "        map_xticks = np.linspace(lon_min_tick, lon_max_tick, num=lon_num_tick, endpoint=True).round(decimals=2) \n",
    "        map_yticks = np.linspace(lat_min_tick, lat_max_tick, num=lat_num_tick, endpoint=True).round(decimals=2)\n",
    "    \n",
    "    #Define cartopy axis\n",
    "    #Use the compressed layout for better spacing subplots\n",
    "    #https://stackoverflow.com/questions/73826115/cartopy-too-much-vertical-space-between-subplots-even-with-tight-layout\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, \n",
    "                            subplot_kw={'projection': plot_crs}, \n",
    "                            gridspec_kw={'wspace':wspace_float, 'hspace':hspace_float}, \n",
    "                            figsize=fig_size, layout='compressed')\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "    #If we we want to make only a single map, put the single \"axs\" into a list so we can iterate it\n",
    "    #Thanks ChatGPT: https://chatgpt.com/share/6702e471-d918-800f-ae87-9a83f77a2939\n",
    "    if nrows*ncols == 1: \n",
    "        axs = [axs]\n",
    "    else:\n",
    "        axs = axs.flatten()\n",
    "    \n",
    "    #For each defined axis we have, create the following basemap\n",
    "    for ax_index, ax in enumerate(axs):\n",
    "\n",
    "        #Set extent of map\n",
    "        ax.set_extent((lon_min_extent, lon_max_extent, lat_min_extent, lat_max_extent), crs=plot_crs)\n",
    "\n",
    "        #Only set x-ticks and y-ticks if the user requested it\n",
    "        if (lon_lat_ticks_on == True) & (xtick_ytick_set_list[ax_index] == True):\n",
    "\n",
    "            #Setting tick locations and labels for lon/lat (degrees; projection = plot_crs)\n",
    "            ax.set_xticks(map_xticks, crs=plot_crs)\n",
    "            ax.set_yticks(map_yticks, crs=plot_crs)\n",
    "            ax.set_xticklabels(map_xticks, **fontdict_tick_labels)\n",
    "            ax.set_yticklabels(map_yticks, **fontdict_tick_labels)\n",
    "\n",
    "            #Add formatter to lon/lat ticks (degree symbol & direction label)\n",
    "            #https://scitools.org.uk/cartopy/docs/latest/gallery/tick_labels.html#sphx-glr-gallery-tick-labels-py\n",
    "            ax.xaxis.set_major_formatter(LONGITUDE_FORMATTER)\n",
    "            ax.yaxis.set_major_formatter(LATITUDE_FORMATTER)\n",
    "            \n",
    "            #Make ticks longer\n",
    "            ax.tick_params(axis='both', which='major', length=15) \n",
    "#--------------------------------------------------------------------------------------------------              \n",
    "        #If the user requested to add WRF topography over southern California to the map, do it!\n",
    "        if (high_res_wrf_topo_sb_bool == True) | (low_res_wrf_topo_sb_bool == True) | (low_res_wrf_topo_ca_bool == True):\n",
    "\n",
    "            #If the user wants the 50m resolution topography, use this file path\n",
    "            if high_res_wrf_topo_sb_bool == True:\n",
    "                wrf_file_str = './SWEX2022_datasets/geographic_files/sb_wrf_topo_50m.nc'\n",
    "                wrf_hgt_str  = 'HGT_M' \n",
    "                wrf_lon_str  = 'XLONG_M'\n",
    "                wrf_lat_str  = 'XLAT_M'\n",
    "                elevation_colorbar_start = 0\n",
    "                elevation_colorbar_end   = 2200\n",
    "                elevation_colorbar_step  = 100\n",
    "                elevation_colorbar_label_step = 1\n",
    "\n",
    "            #Else, if the user wants the 1km resolution topography, use this file path\n",
    "            elif low_res_wrf_topo_sb_bool == True:\n",
    "                wrf_file_str = './SWEX2022_datasets/geographic_files/sb_wrf_topo_1km.nc'\n",
    "                wrf_hgt_str  = 'HGT' \n",
    "                wrf_lon_str  = 'XLONG'\n",
    "                wrf_lat_str  = 'XLAT'\n",
    "                elevation_colorbar_start = 0\n",
    "                elevation_colorbar_end   = 2200\n",
    "                elevation_colorbar_step  = 100\n",
    "                elevation_colorbar_label_step = 1\n",
    "\n",
    "            #Else, if the user wants 9km resolution topography over the entire state of CA, use this path\n",
    "            elif low_res_wrf_topo_ca_bool == True:\n",
    "                wrf_file_str = '/home/voyager-sbarc/wrf/wrf451/sundowners/swex2022/iop10/run_545_ERA5_1km/wrfout_d01_2022-05-11_18:00:00'\n",
    "                wrf_hgt_str  = 'HGT' \n",
    "                wrf_lon_str  = 'XLONG'\n",
    "                wrf_lat_str  = 'XLAT'\n",
    "                elevation_colorbar_start = 0\n",
    "                elevation_colorbar_end   = 3600\n",
    "                elevation_colorbar_step  = 100\n",
    "                elevation_colorbar_label_step = 2\n",
    "\n",
    "            #Read in NetCDF file\n",
    "            nc_file = xr.open_dataset(wrf_file_str)\n",
    "            \n",
    "            # Check if \"Time\" exists in the dataset dimensions and select the first time step if it does\n",
    "            if \"Time\" in nc_file.dims:\n",
    "                nc_file = nc_file.isel(Time=0)\n",
    "            \n",
    "            #Read in NetCDF variables\n",
    "            nc_ele  = nc_file[wrf_hgt_str].squeeze()  #Units = meters\n",
    "            nc_lon  = nc_file[wrf_lon_str].squeeze()  #Units = degrees \n",
    "            nc_lat  = nc_file[wrf_lat_str].squeeze()  #Units = degress \n",
    "\n",
    "            #Make a colormap for topography. Thanks Deanna!\n",
    "            #https://github.com/dlnash/AR_types/blob/master/analysis/fig1_elevation.ipynb\n",
    "            colors_land = plt.cm.terrain(np.linspace(0.35, 1, 256))\n",
    "            terrain_map = mcolors.LinearSegmentedColormap.from_list('terrain_map', colors_land)\n",
    "\n",
    "            #Make the norm\n",
    "            norm_boundaries = np.arange(elevation_colorbar_start, elevation_colorbar_end, elevation_colorbar_step)\n",
    "            norm            = mcolors.BoundaryNorm(boundaries=norm_boundaries, ncolors=256)\n",
    "\n",
    "            #Plot topography\n",
    "            topo_plot = ax.pcolormesh(nc_lon, nc_lat, nc_ele, cmap=terrain_map, shading='auto', norm=norm, transform=data_crs)\n",
    "\n",
    "            #If the user requested a colorbar for elevation on the current plot, add one\n",
    "            if wrf_topo_colorbar_each_plot_bool == True:\n",
    "\n",
    "                #Colorbar for topography\n",
    "                #https://matplotlib.org/3.1.1/gallery/axes_grid1/demo_colorbar_with_axes_divider.html\n",
    "                #Second Answer: https://stackoverflow.com/questions/30030328/correct-placement-of-colorbar-relative-to-geo-axes-cartopy\n",
    "                divider = make_axes_locatable(ax)\n",
    "                cax     = divider.append_axes('right', size='2%', pad=0.25, axes_class=plt.Axes)\n",
    "                cbar    = plt.colorbar(topo_plot, cax=cax, orientation='vertical', spacing='uniform', drawedges=True, ticks=norm_boundaries[::elevation_colorbar_label_step])\n",
    "                cbar.set_label('Elevation (m)', color='black',  labelpad=20, **fontdict_text_color_bar)\n",
    "\n",
    "                #Set font for colorbar tick lables\n",
    "                #https://stackoverflow.com/questions/7257372/set-font-properties-to-tick-labels-with-matplot-lib/7280803\n",
    "                ticks_font = matplotlib.font_manager.FontProperties(family='Nimbus Roman', style='normal', size=24, weight='normal', stretch='normal')\n",
    "                for label in cbar.ax.get_yticklabels():\n",
    "                    label.set_fontproperties(ticks_font)\n",
    "#--------------------------------------------------------------------------------------------------   \n",
    "        #If the user requests a high resolution coastline, draw it.\n",
    "        if high_res_coastline == True:\n",
    "\n",
    "            #Add in high resolution coastline from OSM\n",
    "            reader_ocean     = shapereader.Reader('./data_swex/geographic_plotting_files/osm_ocean/water_polygons_Clip.shp')\n",
    "            reader_coastline = shapereader.Reader('./data_swex/geographic_plotting_files/osm_coastline/west_coast_lines.shp')\n",
    "            ocean            = ax.add_geometries(geoms=reader_ocean.geometries(),     crs=ccrs.PlateCarree(), facecolor=ocean_color, edgecolor='None')\n",
    "            coastline        = ax.add_geometries(geoms=reader_coastline.geometries(), crs=ccrs.PlateCarree(), facecolor='None',      edgecolor='black')\n",
    "\n",
    "        #Else, use natural earth 10m data.\n",
    "        else:\n",
    "            #Add in ocean mask and coastline from Natural Earth (https://www.naturalearthdata.com/)\n",
    "            #https://stackoverflow.com/questions/20990381/how-to-add-custom-shapefile-to-map-using-cartopy\n",
    "            #Add in ocean mask, high resolution coastline, islands, state lines, and country lines all from Natural Earth\n",
    "            #https://www.naturalearthdata.com/\n",
    "            ax.add_feature(cfeature.OCEAN, facecolor=ocean_color, zorder=1)\n",
    "            #ax.add_feature(cfeature.NaturalEarthFeature(category='physical', name='coastline',         scale='10m', facecolor='None', linewidth=2))\n",
    "            ax.add_feature(cfeature.NaturalEarthFeature(category='physical', name='minor_islands',     scale='10m', facecolor='None'))\n",
    "            #ax.add_feature(cfeature.NaturalEarthFeature(category='cultural', name='admin_0_countries', scale='10m', facecolor='None'))\n",
    "            ax.add_feature(cfeature.NaturalEarthFeature(category='cultural', name='admin_1_states_provinces', scale='10m', facecolor='None'))\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "        #If the user requested a scale bar, add one:\n",
    "        if scale_bar_bool == True: \n",
    "\n",
    "            #Add in scalebar\n",
    "            #Scale bar module copied from: \n",
    "            #https://stackoverflow.com/questions/32333870/how-can-i-show-a-km-ruler-on-a-cartopy-matplotlib-plot\n",
    "            scale_bar_text_kwargs = {'fontsize': 24, 'fontweight': 'normal', 'fontname': 'Nimbus Roman'}\n",
    "            scalebar.scale_bar(ax, scale_bar_position, scale_bar_length, color='black', text_kwargs=scale_bar_text_kwargs, linewidth=4, zorder=10)\n",
    "#--------------------------------------------------------------------------------------------------           \n",
    "    #If the user requested one colorbar for elevation for the entire figure, add one\n",
    "    if wrf_topo_colorbar_entire_figure_bool == True:\n",
    "\n",
    "        #Colorbar for topography\n",
    "        #https://matplotlib.org/3.1.1/gallery/axes_grid1/demo_colorbar_with_axes_divider.html\n",
    "        #Second Answer: https://stackoverflow.com/questions/30030328/correct-placement-of-colorbar-relative-to-geo-axes-cartopy\n",
    "        cbar = fig.colorbar(topo_plot, ax=axs.ravel().tolist(), orientation='vertical', spacing='uniform', pad=0.01, drawedges=True, ticks=norm_boundaries[::elevation_colorbar_label_step])\n",
    "        cbar.set_label('Elevation (m)', color='black', **fontdict_text_color_bar)  \n",
    "\n",
    "        #Set font for colorbar tick lables\n",
    "        #https://stackoverflow.com/questions/7257372/set-font-properties-to-tick-labels-with-matplot-lib/7280803\n",
    "        ticks_font = matplotlib.font_manager.FontProperties(family='Nimbus Roman', style='normal', size=24, weight='normal', stretch='normal')\n",
    "        for label in cbar.ax.get_yticklabels():\n",
    "            label.set_fontproperties(ticks_font)\n",
    "#--------------------------------------------------------------------------------------------------       \n",
    "    #If the user requested a inset California figure, add one:\n",
    "    if inset_ca_bool == True: \n",
    "        \n",
    "        #Add inset figure of CA\n",
    "        #https://stackoverflow.com/questions/55385515/embed-small-map-cartopy-on-matplotlib-figure\n",
    "        ax2 = inset_axes(ax, width='100%',height='100%',loc='upper left', borderpad=0,\n",
    "                         axes_class=GeoAxes, axes_kwargs={'projection':plot_crs},\n",
    "                         bbox_to_anchor=inset_bbox_position, bbox_transform=ax.transAxes)\n",
    "\n",
    "        #Set limits of figure (California state)\n",
    "        ax2.set_extent((-126,-114,31,43), crs=plot_crs)\n",
    "\n",
    "        #Add in states and land (Natural Earth Raster)\n",
    "        #Note that we change the maximum amount of pixels that the PIL package can display because if we do not, we get a security error\n",
    "        #See stackoverflow thread linked below for more information\n",
    "        #https://gis.stackexchange.com/questions/313490/increasing-resolution-of-cartopy-stock-background\n",
    "        #https://stackoverflow.com/questions/51152059/pillow-in-python-wont-let-me-open-image-exceeds-limit\n",
    "        # PIL.Image.MAX_IMAGE_PIXELS = 243280000\n",
    "        # ax2.imshow(plt.imread('./data_swex/geographic_plotting_files/NE1_HR_LC_SR_W_DR/NE1_HR_LC_SR_W_DR.tif'), origin='upper', transform=ccrs.PlateCarree(), extent=[-180, 180, -90, 90])\n",
    "        # ax2.add_feature(cfeature.STATES.with_scale('10m'), facecolor='None', edgecolor='black')\n",
    "        ax2.add_feature(cfeature.OCEAN, facecolor='lightskyblue')\n",
    "        ax2.add_feature(cfeature.NaturalEarthFeature(category='cultural', name='admin_1_states_provinces', scale='10m', facecolor='Grey', edgecolor='black'))\n",
    "\n",
    "        #Remove ticks from axis\n",
    "        ax2.tick_params(labelleft=False, labelbottom=False,left=False,bottom=False)\n",
    "\n",
    "        #Add rectangle around zoomed region\n",
    "        #https://scitools.org.uk/cartopy/docs/v0.5/matplotlib/introductory_examples/02.polygon.html\n",
    "        ax2.add_patch(mpatches.Rectangle(xy=[lon_min_extent, lat_min_extent], \n",
    "                                         width=abs((lon_min_extent - lon_max_extent)), \n",
    "                                         height=abs((lat_min_extent-lat_max_extent)), \n",
    "                                         edgecolor='red', fill=False, alpha=1, zorder=4, transform=plot_crs))\n",
    "#--------------------------------------------------------------------------------------------------    \n",
    "    #Return single axis if we only want 1 plot\n",
    "    if nrows*ncols == 1: \n",
    "        return (fig, axs[0])\n",
    "    #Else return a list of axes\n",
    "    else:\n",
    "        return (fig, axs)\n",
    "#---------------------------------------------------------------------------------------------------   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function: \"xy_lidar_wind_profiles_ds\"**\n",
    "- #### Description: This function reads in horizontal wind lidar data collected from the SWEX campaign,and saves each variable into its own array-like (list) variable and then formats the data so it is \"stacked\". To clarify this point, it is important to note that each timestep for all lidar data represents a sample of the vertical profile of the atmosphere. Because we want to eventually create a time-height plot of the data, we stack each vertical level of the data so each layer represents a complete timeseries of that particular layer.  \n",
    "> - #### **Input Parameters:** \n",
    ">> - #### **\"glob_paths\"**: A glob of paths to our lidar files (type: list)\n",
    ">> - #### **\"instituion_str\"**: A string that determines which institution's LiDAR data we are reading in. Options are: \n",
    ">>> - #### **San Jose State University LiDAR = \"sjsu\" (Processed Wind Profile Files)**\n",
    ">>> - #### **National Center for Atmospheric Research LiDARs = \"ncar\" (NetCDF files)**\n",
    ">>> - #### **University of Notre Dame LiDARs = \"und\" (Processed Wind Profile Files)** \n",
    ">>> - #### **University of Virginia LiDAR = \"uwow\" (Processed Wind Profile Files) or \"uwow_vad\" (Processed Velocity-Azimuth Display Profiles)** \n",
    "> - #### **Ouput Parameters:** \n",
    ">> - #### **lidar_zy_profiles_ds**: A xarray dataset that contains horizontal velocitydata and signal-to-noise ratio (UWOW VAD only) as data variables for a single LiDAR instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "#Import entire packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "#metpy imports\n",
    "from metpy.units import units\n",
    "from metpy.calc import wind_components\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "def xy_lidar_wind_profiles_ds(glob_paths, institution_str):\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "    #Ensure glob paths are strings in a sorted list\n",
    "    glob_paths_sorted = [str(path) for path in sorted(glob_paths)]\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "    #NCAR LIDAR PROCESSING\n",
    "    if institution_str == 'ncar':\n",
    "                \n",
    "        #Important Note\n",
    "        #From quick inspection, NCAR LIDARs feature slightly different scan heights for the same instrument in different daily files.\n",
    "        #This makes things a tad more tediuous to work with.\n",
    "        #In order to overcome this limitation, we will do some processing on the \"height\" coordinate among daily LiDAR files\n",
    "        \n",
    "        #Define empty list to store scan heights from each file\n",
    "        height_list = []\n",
    "        \n",
    "        #Define empty lists to store variables we grab from each NetCDF file\n",
    "        xy_wind_u_list         = []\n",
    "        xy_wind_v_list         = []\n",
    "        xy_wind_speed_list     = []\n",
    "        xy_wind_direction_list = []\n",
    "        xy_wind_snr_list       = []\n",
    "        \n",
    "        #For each file that we found, do the following\n",
    "        for file_index, file_path in enumerate(glob_paths_sorted):\n",
    "            \n",
    "            #Open the netcdf ncar lidar file\n",
    "            nc_file = xr.open_dataset(file_path)\n",
    "            \n",
    "            #Save the floored height coordinate for further processing\n",
    "            #We floor the height coordinate becuase this minimizes the amount of conflicting values between daily LiDAR files, while still maintaining accuracy\n",
    "            height_list.append(np.floor(nc_file.height))\n",
    "            \n",
    "            #If we are on the second file or greater, do the following:\n",
    "            if (file_index > 0): \n",
    "            \n",
    "                #Check to see if the current \"height\" coordinate array is close to equal to \n",
    "                if np.allclose(height_list[file_index], height_list[file_index-1], atol=1) == True:\n",
    "                \n",
    "                    #Grab the variables we want (i.e. vertical velocity)\n",
    "                    #Note that we are tranposing this variable to match the dimensions of the date and height 2D variables\n",
    "                    xy_wind_u_list.append(nc_file['u'].transpose())\n",
    "                    xy_wind_v_list.append(nc_file['v'].transpose())\n",
    "                    xy_wind_speed_list.append(nc_file['wind_speed'].transpose())\n",
    "                    xy_wind_direction_list.append(nc_file['wind_direction'].transpose())\n",
    "                    xy_wind_snr_list.append(nc_file['mean_snr'].transpose())\n",
    "            \n",
    "            #Otherwise, if we just have a single file, just grab the variables as usual\n",
    "            else:\n",
    "                    #Grab the variables we want (i.e. vertical velocity)\n",
    "                    #Note that we are tranposing this variable to match the dimensions of the date and height 2D variables\n",
    "                    xy_wind_u_list.append(nc_file['u'].transpose())\n",
    "                    xy_wind_v_list.append(nc_file['v'].transpose())\n",
    "                    xy_wind_speed_list.append(nc_file['wind_speed'].transpose())\n",
    "                    xy_wind_direction_list.append(nc_file['wind_direction'].transpose())\n",
    "                    xy_wind_snr_list.append(nc_file['mean_snr'].transpose())\n",
    "\n",
    "        #Finally, concatenate the variables along the time dimension and simply use the first (left) \"height\" coordinate at the final coordinate\n",
    "        xy_wind_u_concat         = xr.concat(xy_wind_u_list, dim='time', join='override')\n",
    "        xy_wind_v_concat         = xr.concat(xy_wind_v_list, dim='time', join='override')\n",
    "        xy_wind_speed_concat     = xr.concat(xy_wind_speed_list, dim='time', join='override')\n",
    "        xy_wind_direction_concat = xr.concat(xy_wind_direction_list, dim='time', join='override')\n",
    "        xy_wind_snr_concat = xr.concat(xy_wind_snr_list, dim='time', join='override')\n",
    "\n",
    "        \n",
    "        #Create an xarray Dataset from the DataArrays\n",
    "        lidar_xy_profiles_ds = xr.Dataset({'xy_wind_u':xy_wind_u_concat, 'xy_wind_v':xy_wind_v_concat,\n",
    "                                          'xy_wind_speed':xy_wind_speed_concat, 'xy_wind_direction':xy_wind_direction_concat,\n",
    "                                          'xy_wind_snr': xy_wind_snr_concat})\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "    #SJSU, UND, and UWOW (processed wind profiles or VAD files) LIDAR PROCESSING \n",
    "    #Each of these LiDAR instruments have similar data formats\n",
    "    #So we can process them more or less the same\n",
    "    if (institution_str == 'sjsu') | (institution_str == 'und') | (institution_str == 'uwow') | (institution_str == 'uwow_vad'):\n",
    "        \n",
    "        #If we have data from the SJSU, UND, or the non-VAD UWOW instruments, do the following:\n",
    "        if (institution_str == 'sjsu') | (institution_str == 'und') | (institution_str == 'uwow'):\n",
    "            \n",
    "            #Define input parameters for these instruments when reading files using pandas\n",
    "            whitespace_bool = True\n",
    "            skip_rows       = 1\n",
    "            column_names    = ['height_m', 'wind_direction_deg', 'wind_speed_ms']\n",
    "        \n",
    "        #If we have data from UWOW VAD analysus, do the following:\n",
    "        elif institution_str == 'uwow_vad':\n",
    "            \n",
    "            #Define input parameters for these instruments when reading files using pandas\n",
    "            whitespace_bool = False\n",
    "            skip_rows       = 3\n",
    "            column_names    = ['height_m', 'wind_speed_ms', 'wind_direction_deg', 'snr']\n",
    "            xy_snr_single_timestep = []\n",
    "\n",
    "        #Set up storage locations for each timestep of data\n",
    "        xy_date_single_timestep           = []\n",
    "        xy_height_single_timestep         = []\n",
    "        xy_wind_u_single_timestep         = []\n",
    "        xy_wind_v_single_timestep         = []\n",
    "        xy_wind_speed_single_timestep     = []\n",
    "        xy_wind_direction_single_timestep = []\n",
    "        \n",
    "        #For each file that we found, do the following\n",
    "        for file_index, file_path in enumerate(glob_paths_sorted):\n",
    "            \n",
    "            #Read in csv LiDAR file depending on which data we have\n",
    "            df = pd.read_csv(file_path, delim_whitespace=whitespace_bool, skiprows=skip_rows, names=column_names)\n",
    "\n",
    "            #Compute U and V components of wind using metpy and store values in appropriate lists\n",
    "            xy_wind_u, xy_wind_v = wind_components(df['wind_speed_ms'].values*units('m/s'), df['wind_direction_deg'].values*units.deg)\n",
    "            xy_wind_u_single_timestep.append(xy_wind_u)\n",
    "            xy_wind_v_single_timestep.append(xy_wind_v)\n",
    "\n",
    "            #Store each column of data (i.e. 1-dimensional array) in each our our storage locations\n",
    "            #Later on we will reshape our storage locations to plot using the pcolormesh function in matplotlib\n",
    "            xy_date_single_timestep.append(pd.to_datetime(file_path[-19:-11]+file_path[-10:-4]))\n",
    "            xy_height_single_timestep.append(df['height_m'].values)\n",
    "            xy_wind_speed_single_timestep.append(df['wind_speed_ms'].values)\n",
    "            xy_wind_direction_single_timestep.append(df['wind_direction_deg'].values)\n",
    "            \n",
    "            #Do some additional processing for uwow data since it has SNR values\n",
    "            if institution_str == 'uwow_vad':\n",
    "                xy_snr_single_timestep.append(df['snr'].values)\n",
    "            \n",
    "        #Turn our lists of 1D arrays (1 array per timstep per variable) into 2 dimensional arrays\n",
    "        xy_wind_u_2d         = np.column_stack(xy_wind_u_single_timestep)\n",
    "        xy_wind_v_2d         = np.column_stack(xy_wind_v_single_timestep)\n",
    "        xy_wind_speed_2d     = np.column_stack(xy_wind_speed_single_timestep)\n",
    "        xy_wind_direction_2d = np.column_stack(xy_wind_direction_single_timestep)\n",
    "        \n",
    "        #If we have the UWOW VAD do the following final steps\n",
    "        if institution_str == 'uwow_vad':\n",
    "            \n",
    "            #Convert SNR data from 1D to 2D\n",
    "            xy_snr_2d = np.column_stack(xy_snr_single_timestep)\n",
    "            \n",
    "            #Create a xarray Dataset using our variables\n",
    "            lidar_xy_profiles_ds = xr.Dataset({'xy_wind_u':(['height', 'time'], xy_wind_u_2d),\n",
    "                                               'xy_wind_v':(['height', 'time'], xy_wind_v_2d),\n",
    "                                               'xy_wind_speed':(['height', 'time'], xy_wind_speed_2d),\n",
    "                                               'xy_wind_direction':(['height', 'time'], xy_wind_direction_2d),\n",
    "                                               'xy_wind_snr':(['height', 'time'], xy_snr_2d)}, \n",
    "                                             coords={'height':np.unique(np.concatenate(xy_height_single_timestep)), 'time':xy_date_single_timestep})\n",
    "        else:\n",
    "        \n",
    "            #Create a xarray Dataset using our variables (no SNR)\n",
    "            lidar_xy_profiles_ds = xr.Dataset({'xy_wind_u':(['height', 'time'], xy_wind_u_2d),\n",
    "                                               'xy_wind_v':(['height', 'time'], xy_wind_v_2d),\n",
    "                                               'xy_wind_speed':(['height', 'time'], xy_wind_speed_2d),\n",
    "                                               'xy_wind_direction':(['height', 'time'], xy_wind_direction_2d)}, \n",
    "                                             coords={'height':np.unique(np.concatenate(xy_height_single_timestep)), 'time':xy_date_single_timestep})\n",
    "#----------------------------------------------------------------------------------------------------------------------        \n",
    "    return (lidar_xy_profiles_ds)\n",
    "#----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function: \"z_lidar_wind_profiles_ds**\"\n",
    "- #### Description: This function reads LiDAR stare files, which contain vertical velocity data, collected from the SWEX campaign. The function reads stare files from an individual LiDAR and creates an xarray Dataset with the vertical velocity magnitude and signal-to-noise ratio as data variables. \n",
    "> - #### **Input Parameters**: \n",
    ">> - #### **\"glob_paths\"**: A list of file paths to our LiDAR files\n",
    ">> - #### **\"institution_str\"**: A string that defines which LiDAR instrument we are reading data files from. Options are: **\"sjsu\", \"ncar\", \"und\", \"uwow\"**. \n",
    "> - #### **Ouput Parameters**: \n",
    ">> - #### **lidar_z_profiles_ds**: A xarray dataset that contains vertical velocity magnitude and signal-to-noise ratio as data variables for a single LiDAR instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "#Import entire packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "#metpy imports\n",
    "from metpy.units import units\n",
    "from metpy.calc import wind_components\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "def z_lidar_wind_profiles_ds(glob_paths, institution_str):\n",
    "\n",
    "    #Ensure glob paths are strings in a sorted list\n",
    "    glob_paths_sorted = [str(path) for path in sorted(glob_paths)]\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "    #NCAR LIDAR PROCESSING\n",
    "    if institution_str == 'ncar':\n",
    "        \n",
    "        #Important Note\n",
    "        #From quick inspection, NCAR LIDARs feature slightly different scan heights for the same instrument in different daily files.\n",
    "        #This makes things a tad more tediuous to work with.\n",
    "        #In order to overcome this limitation, we will do some processing on the \"height\" coordinate among daily LiDAR files\n",
    "        \n",
    "        #Define empty list to store scan heights from each file\n",
    "        height_list = []\n",
    "        \n",
    "        #Define empty lists to store variables we grab from each NetCDF file\n",
    "        z_wind_speed_list = []\n",
    "        z_wind_snr_list   = []\n",
    "        \n",
    "        #For each file that we found, do the following\n",
    "        for file_index, file_path in enumerate(glob_paths_sorted):\n",
    "            \n",
    "            #Open the netcdf ncar lidar file\n",
    "            nc_file = xr.open_dataset(file_path)\n",
    "            \n",
    "            #Save the floored height coordinate for further processing\n",
    "            #We floor the height coordinate becuase this minimizes the amount of conflicting values between daily LiDAR files, while still maintaining accuracy\n",
    "            height_list.append(np.floor(nc_file.height))\n",
    "            \n",
    "            #If we are on the second file or greater, do the following:\n",
    "            if (file_index > 0): \n",
    "            \n",
    "                #Check to see if the current \"height\" coordinate array is close to equal to \n",
    "                if np.allclose(height_list[file_index], height_list[file_index-1], atol=1) == True:\n",
    "                \n",
    "                    #Grab the variables we want (i.e. vertical velocity)\n",
    "                    #Note that we are tranposing this variable to match the dimensions of the date and height 2D variables\n",
    "                    z_wind_speed_list.append(nc_file['w'].transpose())\n",
    "                    z_wind_snr_list.append(nc_file['mean_snr'].transpose())\n",
    "\n",
    "            #Otherwise, if we just have a single file, just grab the variables as usual\n",
    "            else:\n",
    "\n",
    "                #Grab the variables we want (i.e. vertical velocity)\n",
    "                #Note that we are tranposing this variable to match the dimensions of the date and height 2D variables\n",
    "                z_wind_speed_list.append(nc_file['w'].transpose())\n",
    "                z_wind_snr_list.append(nc_file['mean_snr'].transpose())\n",
    "\n",
    "        #Finally, concatenate the variables along the time dimension and simply use the first (left) \"height\" coordinate at the final coordinate\n",
    "        z_wind_speed_concat = xr.concat(z_wind_speed_list, dim='time', join='override')\n",
    "        z_wind_snr_concat   = xr.concat(z_wind_snr_list, dim='time', join='override')\n",
    "        \n",
    "        #Create an xarray Dataset from the DataArrays\n",
    "        lidar_z_profiles_ds = xr.Dataset({'wind_speed_z':z_wind_speed_concat, 'wind_speed_z_snr':z_wind_snr_concat})\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "    #SJSU, UND, and UWOW LIDAR PROCESSING\n",
    "    if (institution_str == 'sjsu') | (institution_str == 'und') | (institution_str == 'uwow'):\n",
    "        \n",
    "        #We need to define a few processing parameters depending on which institution's LiDAR instrument we are using\n",
    "        if institution_str == 'sjsu':\n",
    "            skip_rows  = 17 \n",
    "            chunk_size = 1345\n",
    "            range_gate_size = 18\n",
    "        elif institution_str == 'und':\n",
    "            skip_rows  = 17 \n",
    "            chunk_size = 151\n",
    "            range_gate_size = 48\n",
    "        elif institution_str == 'uwow':\n",
    "            skip_rows  = 17 \n",
    "            chunk_size = 201\n",
    "            range_gate_size = 30\n",
    "\n",
    "        #Define storage locations for each timestep of observations for each variable\n",
    "        z_snr_chunk_list        = []\n",
    "        z_date_chunk_list       = []\n",
    "        z_height_chunk_list     = []\n",
    "        z_wind_speed_chunk_list = []\n",
    "\n",
    "        #For each stare file we have from the lidar, do the folloing\n",
    "        for file_index, file in enumerate(glob_paths_sorted):\n",
    "            \n",
    "            #Read in the hourly stare file\n",
    "            #We skip the first 17 rows of data for the lidar, since this just contains metadata related to the lidar and stare file\n",
    "            #Also note that we read in an entire hourly stare file, which contains the data for an entire hour of observations (sampled at some frequency. Seems like ~ every 1 second), in chunks\n",
    "            #We read in the data in chunks because it makes dealing with the file structure of each stare a bit more clear\n",
    "            #This is because each stare file contains a row of 5 columns which specifies the lidar scanning information and provides an observation time \n",
    "            #This row of 5 columns is then followed by 200 rows of 4 columns of actual measured data (e.g. vertical velocity)\n",
    "            #This then repeats for every observation that is in the single stare file (observations are collected about every few seconds)\n",
    "            #This is a bit confusing but should not cause any mistakes or error. Check out the raw files to get a better sense of what is going on if these comments are not clear enough.\n",
    "            chunky_df = pd.read_csv(file, names=['time_or_range_gate', 'azimuth_or_vertical_velocity', 'elevation_or_snr_plus_1', 'pitch_or_beta', 'roll_or_nan'], skiprows=skip_rows, chunksize=chunk_size, delim_whitespace=True, on_bad_lines='warn')\n",
    "            \n",
    "            #Because we read each individual file in chunks, we must iterate through the chunks\n",
    "            #For each chunk, do the following\n",
    "            for chunk_index, chunk in enumerate(chunky_df):\n",
    "                \n",
    "                #We found some data in some LiDAR stare files that had some incomplete data/weird symbols\n",
    "                #These chunks end up messing up the way we read in our data since the amount of lines we read in per chunk are affected\n",
    "                #The line below is one way to deal with this, where we simply skip the rest of the entire file if we encounter a single sample that is off\n",
    "                #If the first element \"roll_or_nan\" column in our chunk has a NaN value in the first row, we know we have hit the part of the file where the data is affected\n",
    "                #If this happens skip this iteration. This will also skip all other iterations in the file since the chunking pattern is screwed up\n",
    "                if np.isnan(chunk['roll_or_nan'].iloc[0]):\n",
    "                    continue\n",
    "                else:\n",
    "                    #Make a timestamp that represent the time of each observation\n",
    "\n",
    "                    #Year, month, day of observation come from file name\n",
    "                    #Format of date in file name is \"YYYYMMDD\"\n",
    "                    z_year_month_day_chunk = pd.to_datetime(file[-15:-7], format='%Y%m%d')\n",
    "\n",
    "                    #The rest of the timestamp (hours, minutes, seconds, etc...) comes from the chunk we are on in the stare file\n",
    "                    #This time is provided as a decimal time (e.g. 15.02572123; format is HH.HHHHHHHH)\n",
    "                    #We convert this into a pandas timedelta object, which will convert the decimal into the appropriate hours, minutes, seconds we need\n",
    "                    #We then can easily combine this with the year, month, day datetime object we created before\n",
    "                    z_decimal_hour_chunk   = pd.Timedelta(chunk['time_or_range_gate'].iloc[0], unit='hours')\n",
    "\n",
    "                    #Combine year, month, day datetim eobject from file name with Timedelta object from stare file to get actual observation time for a single observation \n",
    "                    z_date_single_chunk = z_year_month_day_chunk + z_decimal_hour_chunk\n",
    "\n",
    "                    #Convert column of range gate numbers into altitudes\n",
    "                    #From the stare file metadata, we see that the altitude of each measurement is defined as:\n",
    "                    #Altitude of Measurement = (range_gate_number * 0.5) + gate_length, where gate_length is a constant value each instrument's stare files\n",
    "                    #Also notice how we index this series of values, starting with the second value (remember python uses zero indexing) and going to the end of the chunk\n",
    "                    #This is because we do not want the first row of data which contains 5 columns of values that correspond to lidar scanning and time information\n",
    "                    z_height_single_chunk = (chunk['time_or_range_gate'].iloc[1::] + 0.5) * range_gate_size\n",
    "\n",
    "                    #Grab the vertical velocity column, be sure to skip the first row\n",
    "                    z_wind_speed_single_chunk = chunk['azimuth_or_vertical_velocity'].iloc[1::]\n",
    "\n",
    "                    #Grab signal to noise ratio (SNR) values\n",
    "                    #Remember that in the stare files we have a column of intensity values which are defined as SNR+1\n",
    "                    #To get the actual SNR values, we subtract 1 from this column\n",
    "                    z_snr_single_chunk = chunk['elevation_or_snr_plus_1'].iloc[1::] - 1\n",
    "\n",
    "                    #Append all variable values for a single observation time to the proper storage locations\n",
    "                    z_date_chunk_list.append(z_date_single_chunk)\n",
    "                    z_height_chunk_list.append(z_height_single_chunk.values)\n",
    "                    z_wind_speed_chunk_list.append(z_wind_speed_single_chunk.values)\n",
    "                    z_snr_chunk_list.append(z_snr_single_chunk.values)\n",
    "        \n",
    "        #Turn lists of 1-D arrays into 2-D arrays\n",
    "        z_wind_speed_2d = np.column_stack(z_wind_speed_chunk_list)\n",
    "        z_wind_snr_2d   = np.column_stack(z_snr_chunk_list)\n",
    "        \n",
    "        #Create a xarray Dataset using our variables\n",
    "        lidar_z_profiles_ds = xr.Dataset({'wind_speed_z':(['height', 'time'], z_wind_speed_2d), \n",
    "                                          'wind_speed_z_snr':(['height', 'time'], z_wind_snr_2d)}, \n",
    "                                         coords={'height':np.unique(np.concatenate(z_height_chunk_list)), 'time':z_date_chunk_list})\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "    #Return DataFrame\n",
    "    return(lidar_z_profiles_ds)\n",
    "#----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a helper function \"vertical_resolution_averaging_radiosonde**\"\n",
    "- #### Description: This function reads in some user input parameters, as well as two array-like inputs that should come from a radiosonde, and essentially averages the input variable from the radiosonde to an evenly spaced vertical resolution. Another important distinction is that the location of the vertically averaged radiosonde variable is placed at at the half way point between each vertical resolution bin (\"radiosonde_altitude_array_evenly_spaced_midpoint\" output parameter). \n",
    "- #### For example, if the user requests that the radisonde temperature variable we vertically avaeraged every 25m starting from 0m above ground, the first vertically averaged point would consist of all temperature values that fall between 0m and 25m and that point would be placed half way between 0m and 25m (i.e. 12.5m). This translates to our vertically averaged point being comprised of values above and below its location. \n",
    "> - #### **Input Parameters**: \n",
    ">> - #### **\"starting_altitude\"** | **Type: float** | The starting altitude (in any unit) we want in the beginning of our evenly spaced altitude array (Should always be zero)\n",
    ">> - #### **\"ending_altitude\"** | **Type: float** | The ending altitude (in any unit) we want at the end of our evenly spaced altitude array (10000 m seems to be a good value for SWEX data so far, but we could try others)\n",
    ">> - #### **\"desired_vertical_resolution\"** | **Type: float** | The desired vertical resolution we want to average to (in any units, as long as units are consistent with the variable \"radiosonde_altitude_array\")\n",
    ">> - #### **\"radiosonde_altitude_array\"** | **Type: array-like** | The altitude array (1-dimensional) from the radiosonde data\n",
    ">> - #### **\"radiosonde_met_variable_array\"** | **Type: array-like** | The meteorlogical variable array (1-dimensional) from the radiosonde that we want to modify to match a specific vertical resolution\n",
    ">> - #### **\"radiosonde_altitude_array_evenly_spaced_midpoint_bool\"** | **Type: Boolean** | A boolean that determines if the function returns an altitude array that represents to the midpoints of each vertical averaging window. \n",
    "> - #### **Ouput Parameters**:\n",
    ">> - #### **\"radiosonde_altitude_array_evenly_spaced_midpoint\"** | **Type: numpy array** | An array (1-dimensional) which contains the evenly spaced altitude values for the meteorological variable we have averaged added to the vertical resolution divided by 2 | Returned ONLY if input parameter \"radiosonde_altitude_array_evenly_spaced_midpoint_bool\" is set to true\n",
    ">> - #### **\"radiosonde_metvariable_array_averaged\"** | **Type: numpy array** | An array (1-dimensional) which contains the values of the meteorological variable that has been averaged | Returned regardless of status of input parameter: \"radiosonde_altitude_array_evenly_spaced_midpoint_bool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "#Create a function for averaging radisonde data at a given vertical resolution\n",
    "def vertical_resolution_averaging_radiosonde(starting_altitude, ending_altitude, desired_vertical_resolution, radiosonde_altitude_array, radiosonde_metvariable_array, radiosonde_altitude_array_evenly_spaced_midpoint_bool):\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "    #Define storage location for averaged met variable values\n",
    "    radiosonde_metvariable_array_averaged = []\n",
    "\n",
    "    #Define storage location for altitude values (spaced at midpoint increments)\n",
    "    radiosonde_altitude_array_evenly_spaced_midpoint = []\n",
    "    \n",
    "    #Create the array which represents the altitudes with the desired evenly spaced vertical resolution\n",
    "    #This array will be used to create the altitude array we will use for plotting, which is simply this array but with the altitude values offset by (desired_vertical_resolution/2)\n",
    "    #Example: The requested vertical resolution for our data is 25m, thus for the first point we average all values between 0m and 25m and then plot that averaged value at an altitude of 12.5m\n",
    "    radiosonde_altitude_array_evenly_spaced = np.arange(starting_altitude, ending_altitude+desired_vertical_resolution, desired_vertical_resolution)\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "    #For each value in the radiosonde altitude 1-dimensional array, do the following\n",
    "    for altitude_index, altitude_value in enumerate(radiosonde_altitude_array_evenly_spaced):\n",
    "        \n",
    "        #For the first value take the mean of varibale values between the first altitude in our created array and the one directly above it\n",
    "        #If we want the midpoint_altitude_array, perform that computation as well\n",
    "        if altitude_index == 0:\n",
    "            \n",
    "            #Average met variable array between two altitudes via indexing\n",
    "            radiosonde_metvariable_array_averaged.append(np.nanmean(radiosonde_metvariable_array[(radiosonde_altitude_array >= radiosonde_altitude_array_evenly_spaced[altitude_index]) & \n",
    "                                                                                              (radiosonde_altitude_array <= radiosonde_altitude_array_evenly_spaced[altitude_index+1])])) \n",
    "        \n",
    "        #For all other evenly spaced altitude values, besides that last altitude, take the mean between the current altitude and the next altitude\n",
    "        elif altitude_index < len(radiosonde_altitude_array_evenly_spaced)-1: \n",
    "            \n",
    "            #Average met variable array between two altitudes via indexing\n",
    "            radiosonde_metvariable_array_averaged.append(np.nanmean(radiosonde_metvariable_array[(radiosonde_altitude_array > radiosonde_altitude_array_evenly_spaced[altitude_index]) & \n",
    "                                                                                              (radiosonde_altitude_array <= radiosonde_altitude_array_evenly_spaced[altitude_index+1])]))\n",
    "        \n",
    "        #If the user wants an array of evenly spaced midpoint altitudes, compute it\n",
    "        #If we are on the last evenly spaced altitude value, do not compute that one.\n",
    "        if (radiosonde_altitude_array_evenly_spaced_midpoint_bool == True) & (altitude_index < len(radiosonde_altitude_array_evenly_spaced)-1):\n",
    "                \n",
    "                #Store averaged altitude value added to the vertical resolution divided by 2 (i.e. midpoint) if the user wants it\n",
    "                radiosonde_altitude_array_evenly_spaced_midpoint.append(altitude_value+(desired_vertical_resolution/2))\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "    #Return the evenly spaced midpoint altitude array AND the averaged met variable array if that is what the user requested\n",
    "    if radiosonde_altitude_array_evenly_spaced_midpoint_bool == True:\n",
    "        return(np.asarray(radiosonde_altitude_array_evenly_spaced_midpoint), np.asarray(radiosonde_metvariable_array_averaged))\n",
    "   \n",
    "    #Else return ONLY the averaged met variable array\n",
    "    else:\n",
    "        return(np.asarray(radiosonde_metvariable_array_averaged))\n",
    "#----------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (swex)",
   "language": "python",
   "name": "swex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
